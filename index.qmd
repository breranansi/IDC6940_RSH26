---
title: "Using Symbolic Regression to Model Molecular Mass in Naturally Occurring Hydrocarbon Fluids"
subtitle: ""
author: "Delanyce Rose & Richard Henry (Advisor: Dr. Cohen)"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

Slides: [slides.html](slides.html){target="_blank"} ( Go to `slides.qmd`
to edit)


## Introduction
Molecular mass is an important parameter used to model the physical properties of hydrocarbon fluids.  These fluids, such as gasoline and lubricating oil are essential for so many aspects of modern life, and although these end-products are homogenized to exacting standards, the feed stock properties from which they are made vary significantly from one source to another and from the same source over time.

Unfortunately, molecular mass is expensive to measure accurately, so that it is common practice to use more easily measured parameters to estimate the value of molecular mass.

Many of these correlations [@stratiev2023prediction] have been published over the decades, and although consensus has been reached on *which* predictor variables should be chosen, there is still considerable variation in the *form* of the equation appearing in the literature.

Rapid progress in machine learning technologies in recent years has provided a new toolbox with which we can revisit the problem.  One of these tools is *symbolic regression*.

#### <s>Symbolic Regression</s>

Consider a dataset consisting of an independent variable $X$ and a dependent variable $y$. Symbolic Regression [@koza1994genetic] allows us to discover, from the data, an analytical expression $f(X)$ which we can use to predict values of $y$ for values of $X$ unseen by the algorithm during training. For example:

$\hat{y}_{23} \approx f(X_{23})$

#### <s>Neural Networks</s>

Neural networks allow us to make this prediction without first generating an analytical expression. Often, this is good enough, but we give up a property called *explainability*.

In the case of a neural net, we can reclaim some of this explainability by performing a second prediction for a value of $X$ we really don't care about, but happens to be close to $X_{23}$:

$\hat{y}_{24} \approx f(X_{24})$

And now we can make statements on whether $y$ increases or decreases with increasing $X$ in the neighbourhood of $X_{23}$:

$\frac{dy}{dX}|_{X_{23}}\approx \frac{\hat{y}_{24}-\hat{y}_{23}}{X_{24}-X_{23}}$

#### <s>Linear Regression</s>

The traditional alternative to neural networks which provides explainability is linear regression:

$\hat{y}_{23} \approx \beta_0 +\beta_1 \cdot X$

In which we use the data to discover the best values of the constants $\beta_0$ and $\beta_1$.

However, this may not be the best functional form for this predictive equation. For example, maybe an exponential or a logarithmic relationship may be better:

$\hat{y}_{23} \approx \beta_2 +\beta_3 \cdot e^X$

or

$\hat{y}_{23} \approx \beta_4 +\beta_5 \cdot log(X)$

or maybe

$log(\hat{y}_{23}) \approx \beta_6 +\beta_7 \cdot log(X)$

We can certainly investigate these, and hundreds of other possibilities by building multiple models and comparing the performance between them.

#### <s>Alternative Conditional Expectations</s>

Alternative Conditional Expectations [@breiman1985estimating] is a technology that will automatically transform both the dependent and the independent variables such that the relationship between the dependent variable and a linear combination of the independent variables is as straight as possible.  Accordingly, building and testing hundreds of linear regression models can be avoided. This is different from the Box-Cox transform, where the aim is to make the population more normal.

#### <s>Non-Linear Regression</s>

Although, as shown above, we can use linear regression to find non-linear relationships between variables, some relationships may be challenging, such as:

$\hat{y}\approx\frac{\beta_0+\beta_1x+\beta_2x^2}{\beta_3+\beta_4x}$

Rational fractions like this may require full-blown non-linear regression techniques which will require us to specify the form of the equation and then use the data to find the values of the constants.

The promise of symbolic regression is to find the optimum functional form, which may *not* be linear, *and* the optimum values of the constants in the same workflow.

### Next Steps

In this paper, we will endeavor to:

-   Describe the most common workflow for symbolic regression

-   <s>Describe some of the strategies being used to improve symbolic regression performance</s>

-   <s>Give examples of how a selection of open-source libraries perform on a toy dataset</s>

-   Use symbolic regression to analyze a molecular mass dataset


## Methods

### Typical Workflow

1.  The first step is deciding *how* we are going to encode the equations so that they are easily manipulated in software. The challenges are similar to text processing. For example, we will want equations to be of variable length (like our sentences) and the order of the units making up the equation matter, just as the order of the words making up a sentence do.

2.  The second step is to decide *how* we are going to measure the fitness of a particular equation. The standard workflow is to decode the equation into a function, apply that function to each row in the training dataset, and then calculate the mean squared error between the target value and the equation result.

3.  The third step is to decide *when* we will stop the workflow. Usually we will set a maximum number of generations *and* an error threshold that the best equation has to meet to stop the search. This equation is the one that will be offered-up as the answer at the end of the workflow.

4.  The fourth step is to generate an initial population of candidate equations. The quantity is usually in the thousands, and involves random selection.

5.  After the first fitness evaluation, the quality of the equations are usually quite poor. The best are selected, and then changed randomly to form the next generation, in a process influenced by biological evolution theory.

6.  We then perform the fitness evaluation on the new generation. We are expecting, of course, that each generation will produce better equations than the last, *on average*.

7.  Next, we check that the fitness value of the best equation and/or the number of generations tells us to stop. If not..

8.  Then we again select the best and change them for the next generation. 
![Figure 2: Workflow](generic_workflow.svg)

### Toy Dataset
We are going to build a toy dataset to demonstrate the workflow. In addition to its small size (one predictor variable, 21 rows) the fact that we know *exactly* what the generating equation is gives us a yardstick to compare our results against.  Here is the equation for predicting API gravity from specific gravity:

$\gamma_{API}=\frac{141.5}{\gamma_{o}}-131.5$

|Symbol|Meaning|Units|
|---|---|---|
|$\gamma_{API}$|API Gravity|[degAPI]|
|$\gamma_o$|specific gravity|[1/wtr]|

And here is our dataset:

```{r}
x<-seq(from=0.55,to=1.075,by=0.025)
y<-141.5/x-131.5
plot(x,y,main="First Toy Dataset",xlab="Specific Gravity",ylab="API Gravity")
grid()
```

### Software Implementation
Most libraries available in Python for Symbolic Regression follow the Scikit-Learn model.  We will  use the PySR library [@cranmer2023interpretable] for demonstration purposes.  First, we import the main function for the library:

`%pip install -U pysr`

`from pysr import PySRRegressor()`

Next, we call the main function to set-up the model.  Here, we are using default parameters for everything:

`myMod=PySRRegressor()`

Following that, we instruct the software to search for a suitable equation for the data we have provided:

`myMod.fit(x.reshape(-1, 1),y)`

Here, `x` is specific gravity and `y` is the API Gravity.  The `reshape` is necessary as Scikit-Learn usually expects more than one predictor variable.

Next, we can check the quality of the fit by plotting the predicted values against the original values:

`y_pred=myMod.predict(x.reshape(-1, 1))`

The reader will notice that we have neglected to divide our dataset into `test` and `train` buckets for this demonstration.

```{r}
y_fit<-141.5/x-131.500000554
y_raw<-141.5/x-131.5

plot(x,y_raw,main="Toy Dataset Fit",xlab="Specific Gravity",ylab="API Gravity")
lines(x,y_fit,type="l",lty=1,col="red")
legend("topright",legend=c("Original","Predicted"),col=c("black","red"),lty=c(1,1),pch=c(1,1))
grid()
```

Next, if we like what we see, we can ask to inspect the final equation:

`myEq=myMod.sympy()`

And the evolutionary algorithm will typically present an output looking like this:

$x_0-(x_0+0.013196754)+1.0131962+ \frac{x_0 (-132.5)- -141.5}{x_0}$

This looks terrible, but we can clean it up using a symbolic mathematics library (or high-school algebra):

`sym.simplify(myEq)`

which produces a much better:

$-131.500000554+\frac{141.5}{x_0}$
 
### Noisy Toy

One of the challenges of *real* data is that it is noisy.  Accordingly we will add some noise to our toy dataset:

`w=(random.rand(21)-0.5)*2.5`

and re-run it to see whether symbolic regression is robust enough to handle noise:

![Noisy Toy](/noisy_pysr_files/figure-html/cell-7-output-1.png)

This was enough to throw-off the estimation of the constants a bit, but the form of the equation was preserved:

$-132.05688+\frac{141.88547}{x_0}$

### Belly of the Beast
There are a large number of hyperparameters in a typical symbolic regression model that we have defaulted in the Toy dataset model above.  Here are a few of them:

#### Step 1 (Encoding)

`binary_operators=["+","-","*","/"]`

Here we choose which binary operators may appear in our equations. These operators work on two numbers, for example $a+b$.  On first glance, this may appear to be restrictive, but for example, the ternary operation $a+b+c$ can be chained as two binaries $a+(b+c)$.

A more serious challenge is handling underflow and overflow. In particular, the classic "division-by-zero" error appear very easily when we are randomly assembling equations using operators, variables and constants.  Every library will use a different coping mechanism, but common strategies include pre-assigning a value such as zero, one or positive infinity to a calculation that will generate errors. 

There are additional binary operators such as the power function ($a^b$) or the maximum and minimum functions.

`unary_operators=None`

Here we choose the unary operators that may appear in our equations. Some libraries call these "functions" instead.  Examples include sine, cosine and square root, the latter capable of being represented by a power function mentioned above.

`maxsize=30`

This is the maximum length of a generated equation.  Our toy equation has a length of five:

|1|2|3|4|5|
|---|---|---|---|---|
|minus|divide|141.5|specific gravity|131.5|


`maxdepth=None`

This is the maximum depth of a generated equation.  Our toy equation has a depth of three:
![Toy Tree](API_tree.png)

`None` in this case means that there are no restrictions on the depth of the equation.

#### Step 2 (Fitness)

`elemtwise_loss="L2DistLoss()"`

This switch says that we will used least squares to judge the accuracy of an equation.  There are several canned loss functions like this, for example `L1DistLoss` will use the average absolute difference between the equation predictions and the actual values.  In addition, one may define custom loss functions using julia expressions in PySR.

`parsimony=0.0`

This is a regularization weight.  Unlike Lasso or Ridge regression, the length of the equation is penalized instead of the size of the coefficients. Other libraries consider the length of the equation and over-simplified measure of complexity and propose other measures.

The idea here is that a simple equation that is very nearly as accurate as a complex one is more "explainable", and therefore there needs to be a mechanism to encourage simpler equations.

`model_selection="best"`

This switch says that the models selected will be a specified trade-off between accuracy and complexity.  There are other formulations such as `score` involving log-loss derivatives or one could simply choose `accuracy`.

`should_simplify=True`

This switch says that occasionally equations generated during the search will be simplified.  Superficially, it may appear to be a good idea to simplify the equations after *every* generation, to cut down on "bloat" or equations that are more complex than they need to be.  For example, $x_0+5x_0-2x_0$ vs. just $4x_0$

However, the PySR developers found that allowing some bloat improved the search significantly.

`Tournament_selection_n=15`

In a nod to evolutionary biology, this says that models compete in groups of 15 for selection.  The larger this number, the fewer "winners" there are after every round.

#### Step 3 (Stopping)

`niterations=100`

This is equivalent to an epoch in other machine learning algorithms.  100 is actually quite low. Other libraries require low thousands of epochs to produce the same result.

`max_evals=None`

This switch counts the number of equation evaluations performed, and stops the search after a specified number.  Equation evaluations are a significant fraction of the run time, especially as the equations get longer and contain more transcendental functions.

`timeout_in_seconds=None`

This switch keeps track of the run time and stops the search after a specified number.

#### Step 4 (Population)

`populations=31`

This switch says that we are going to run 31 populations in parallel.  This suggests that at the end of the run we should have 31 *almost* independent "best" solutions to choose from.

In practice, however, some populations "go extinct" which in our context means that they fail to produce reasonable equations. `PySR` does allow some "immigration" between populations, which produces benefits in both directions.  The "better" populations have some diversity injected into them, and the "worse" populations are given new opportunities to succeed.

`population_size=27`

This switch says that each population will have 27 individuals.  Notice that the tournament size is smaller than this, so that there is more than one competition between individual equations.


### Alternatives to Evolution

Although genetic programming has proven to be most popular approach to Symbolic Regression (ref), it has been criticized for being slow and producing bloated equations (ref).

Deterministic methods such as brute-force search (ref), mathematical programming (ref) and sparse regresion (ref) have been employed from the early times until today.  Typically, they don't solve the speed problem, but are thought to produce more interpretable equations.

Methods which rely on information technology have also been used, especially of late.  The idea here is to replace the random changes to the equations that are the heart of the evolutionary approach with changes based on what we know about th data.  Sometimes this information is learned from the data (ref), and other times it is supplied externally (ref) as "human experience".  In the latter case, there is the risk that the search space may become so restricted that nothing new can be learned.

Neural Network methods have also made an appearance.  An early approach (ref) replaced the activation functions in an ANN with trancendental functions such as sine and log.  The main idea here is that each layer would host multiple kinds of functions instead of just one.  The more recent approaches (ref) pre-train a transformer model to learn the relationships between data and equations using synthetic data.  The major advantage here is that if we ignore the pre-training phase, the training phase may be quick enough to use the model in real time.


### Next Steps

1. Examine our dataset and describe some of the equations that have been used historically to calculate molecular mass.

2. Look at the options for generating equations from data in the `PySR` symbolic regression libray.

3. Compare and contrast the equations that are generated by `PySR` for our dataset as the options are varied.


## Analysis and Results

### Data Exploration and Visualization
Let us take a first look at the dataset from Goossens [@goossens1996prediction]:

```{r}
# Positron
#df<-read.csv("./c694/goossens_raw.csv")
# Quarto
df<-read.csv("./goossens_raw.csv")
```

We have 3 variables:

|Variable|Description|Designation|
|---|---|---|
|$Mw$|Molecular Mass|dependent|
|$SG$|Specific Gravity|independent|
|$TBP$|True Boiling Point|independent|

and 70 rows.  This is a very small dataset by modern machine learning standards.  However, due to the cost of aquiring the molecular mass data, this dataset will be considered "large" by chemical engineering standards.

One of the questions we want to answer is whether datasets like this are "too small" for symbolic regression.

Here is a plot of molecular mass vs. specific gravity:

```{r}
plot(df$SG,df$MW,main="Goossens Dataset",xlab="Specific Gravity",ylab="Molecular Mass")
grid()
```

Although there appears to be a clear linear relationship between molecular mass and specific gravity at low gravity numbers, the heteroscedasticity explodes above a gravity of about 0.75.

Here is a plot of molecular mass vs. true boiling point:

```{r}
plot(df$TBP,df$MW,main="Goossens Dataset",xlab="True Boiling Point",ylab="Molecular Mass")
grid()
```

There seems to be a monotonically increasing relationship between molecular mass and true boiling point, with a possible "pole" around the boiling point of 1000.

At this point, it may be tempting to ignore the effect of specific gravity on the prediction of molecular mass.

Here we plot the two independent variables against one another:

```{r}
plot(df$TBP,df$SG,main="Goossens Dataset",xlab="True Boiling Point",ylab="Specific Gravity")
grid()
```

This plot suggests that the correlation is poor between true boiling point and specific gravity.  Presumably, the specific gravity helps to reduce the scatter around the trend of molecular weight vs. true boiling point.

#### Existing Correlations
The existing correlations available for estimating molecular mass from true boiling point and specific gravity provide hints as to what kind of equations we expect to see from the algorithm.  Many of them predate symbolic regression and therefore were developed by people using intuition and experimentation.

Here are a few of them using the following nomenclature:

|Symbol|Meaning|
|---|---|
|$M_w$|Apparent Molecular Mass|
|$T_b$|Boiling Point Temperature at Atmospheric Pressure|
|$\gamma$|Specific Gravity (density compared to water)|
|$a_{00}..a_{09}$|Empirical Constants|
|$K_w$|Characterization factor (intermediate value)|
|$X_0...X_3$|Intermediate Variables|


##### Hariu & Sage (1969)
$M_w = a_{00} + a_{01} K_w + a_{02} K_w^2 + a_{03} T_b K_w + a_04 T_b K_w^2 + a_{05} T_b^2 K_w + a_{06} T_b^2 K_w^2$

$K_w =\frac{\sqrt[3]T_b}{\gamma}$

##### Kesler & Lee (1976)
$M_w = X_0 + \frac{X_1}{T_b} + \frac{X_2}{T_b^2}$

$X_0 = a_{00} + a_{01} γ+ \left (a_{02} + a_{03} γ \right ) T_b$

$X_1 = \left (1+ a_{04} γ + a_{05}γ^2 \right ) \left (a_{06} + \frac{a_ {07}}{T_b} \right ) \cdot 10^7$

$X_2 = \left (1+ a_{08} γ+ a_{09} γ^2 \right ) \left (a_{10} + \frac{a_{11}}{T_b} \right ) \cdot 10^12$

##### American Petroleum Institute (1977)
$M_w = a_{00} e^ {\left (a_{01} T_b \right )} e^{\left (a_{02} γ \right )} T_b^{a_{03}} γ^ {a_{04}}$

##### Winn, Sim & Daubert (1980)
$M_w = a_{00} T_b^{a_ {01}} γ^{a_{02}}$

##### Riazi & Daubert (1980)
$M_w = a_{00} T_b^{a_ {01}}γ^{a_{02}}$

##### Rao & Bardon (1985)
$ln {M_w} = (a_{00} + a_{01} K_w) ln (\frac{T_b} {a_{02} + a_{03} K_w} )$

##### Riazi & Daubert (1987)
$M_w = a_{00} T_b^{a_{01}} γ^{a_{02}} e^{\left (a_{03} T_b + a_{04} γ + a_{05} T_b γ \right )}$

##### Goossens (1996)
$M_w = a_{00} T_b^{X_0}$

$X_0 =\frac {a_{03} + a_{04} ln {\left (\frac{T_b}  {a_{05} - T_b} \right )}}  {a_{01} γ + a_{02}}$

##### Linan (2011)
$M_w = a_{00} e^{\left (a_{01} T_b \right )} e^{\left (a_{02} γ \right )} T_b^ {a_{03}} γ^{a_{04}}$

##### Hosseinifar & Shahverdi (2021)
$M_w = {\left [a_{00} T_b^{a_{01}} {\left (\frac{3+2γ} {3-γ} \right )}^{\frac{a_{02}}{2}} + a_{03} T_b^{a_{04}} {\left (\frac{3+2γ}{3-γ} \right )}^{\frac{a_{05}}{2}} \right ]}^{a_{06}}$

##### Stratiev (2023)
$M_w = a_{00} + a_{01} e^{\left [a_{02} e^{\left (a_{03} \frac{T_b^{a_{06}}}{γ^{a_{05}}} \right )} \right ]}$

The reader will notice that all the correlations are non-linear, and that only a few of them can be easily transformed into a linear relationship. Some of the additional operators we may want to consider include:

|operator|type|description|
|---|---|---|
|pow|binary|one expression raised to the power of another|
|log|unary|logarithm of an expression|
|exp|unary|e raised to the power of an expression|
|sqr|unary|expression squared|
|cub|unary|expression cubed|
|inv|unary|inverse of an expression|


### Modeling and Results

Our first experiment is to run `PySR` with default parameters against our molecular mass dataset.  The resulting equation, of course, looks nothing like the existing equations:

$M_w=a_{00}+\frac{a_{01}}{\gamma-a_{02}}+\frac{T_b(a_{03}T_b-a_{04})}{\gamma(a_{05}-\frac{a_{06}}{T_b})}$

But its performance is quite impressive considering its restricted grammar of addition, subtraction, muliplication and division:

![Default Params](moleDataSet_files/figure-html/cell-11-output-1.png)

It even has a (slightly) better correlation coefficient with the raw molecular mass data than the equation presented *with* this data in the Goosens paper:

<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Raw SG</th>
      <th>Raw TBP</th>
      <th>Raw MW</th>
      <th>Goossens<br>Equation</th>
      <th>This<br>Equation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>Raw SG</th>
      <td>1.000000</td>
      <td>0.625218</td>
      <td>0.334852</td>
      <td>0.339126</td>
      <td>0.337190</td>
    </tr>
    <tr>
      <th>Raw TBP</th>
      <td>0.625218</td>
      <td>1.000000</td>
      <td>0.869591</td>
      <td>0.868486</td>
      <td>0.871037</td>
    </tr>
    <tr>
      <th>Raw MW</th>
      <td>0.334852</td>
      <td>0.869591</td>
      <td>1.000000</td>
      <td>0.999711</td>
      <td>0.999847</td>
    </tr>
    <tr>
      <th>Goossens Equation</th>
      <td>0.339126</td>
      <td>0.868486</td>
      <td>0.999711</td>
      <td>1.000000</td>
      <td>0.999798</td>
    </tr>
    <tr>
      <th>This Equation</th>
      <td>0.337190</td>
      <td>0.871037</td>
      <td>0.999847</td>
      <td>0.999798</td>
      <td>1.000000</td>
    </tr>
  </tbody>
</table>
</div>

There are some challenges. The constant $a_{02}$ is only slightly larger than the largest $\gamma$ (specific gravity) in the dataset.  This limits the extrapolation power of this equation to higher specific gravities than seen in this dataset.

### Conclusion

## References
