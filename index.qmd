---
title: "Using Symbolic Regression to Model Molecular Mass in Naturally Occurring Hydrocarbon Fluids"
subtitle: "Advisor: Dr. A. Cohen"
author: "Delanyce Rose & Richard Henry"
date: '`r Sys.Date()`'
toc: true
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

Slides: [here](shorty.html){target="_blank"}

## Introduction

Molecular mass is an important parameter used to model the physical
properties of hydrocarbon fluids. These fluids, such as gasoline and
lubricating oil are essential for so many aspects of modern life, and
although these end-products are homogenized to exacting standards, the
feed stock properties from which they are made vary significantly from
one source to another and from the same source over time.

Unfortunately, molecular mass is expensive to measure accurately, so
that it is common practice to use more easily measured parameters to
estimate the value of molecular mass.

Many of these correlations [@stratiev2023prediction] have been published
over the decades, and although consensus has been reached on *which*
predictor variables should be chosen, there is still considerable
variation in the *form* of the equation appearing in the literature.

Rapid progress in machine learning technologies in recent years has
provided a new toolbox with which we can revisit the problem. One of
these tools is *symbolic regression* [@koza1994genetic], which had its
origins in an early attempt to teach machines to code, but is now used
to generate equations from data.

This technology was famously used to recreate 100 equations from the
Feynman Lectures in Physics [@udrescu2020ai], and made the jump to
generating differential equations [@bongard2007automated] for describing
engineering and biological systems. The story of Kepler discovering that
the earth's orbit around the sun was *not* circular based on data
collected by Brahe, which upended Ptolemy's conjecture made a thousand
years previously [@brunton2016discovering] is often repeated as an early
example of symbolic regression. Like Kepler, if we are more interested
in understanding the underlying relationships between variables than
prediction accuracy in the presence of noise, then symbolic regression
is a good place to start.

A second use case for symbolic regression concerns predicting the
behaviour of systems beyond the limits of the data collected on that
behaviour. Researchers at the Institute of Science and Technology
[@martius2016extrapolation] in Austria were interested in predicting the
performance of a robotic arm when it is operated outside of its
specifications. The hypothesis was that an equation-based
*extrapolation* will fail less spectacularly than a black-box one.

## Methods

Consider a dataset consisting of an independent variable $X$ and a
dependent variable $y$. Symbolic Regression [@koza1994genetic] allows us
to discover, from the data, an analytical expression $f(X)$ which we can
use to predict values of $y$ for values of $X$ unseen by the algorithm
during training. For example:

$$\hat{y}_{23} \approx f(X_{23})$$ {#eq-gen}

Neural networks allow us to make this prediction without first
generating an analytical expression. Often, this is good enough, but we
give up a property called *explainability*.

In the case of a neural net, we can reclaim some of this explainability
by performing a second prediction for a value of $X$ we really don't
care about, but happens to be close to $X_{23}$:

$$\hat{y}_{24} \approx f(X_{24})$$ {#eq-plus}

And now we can make statements on whether $y$ increases or decreases
with increasing $X$ in the neighbourhood of $X_{23}$:

$$\frac{dy}{dX}|_{X_{23}}\approx \frac{\hat{y}_{24}-\hat{y}_{23}}{X_{24}-X_{23}}$$ {#eq-slope}

The traditional alternative to neural networks which provides
explainability is linear regression:

$$\hat{y}_{23} \approx \beta_0 +\beta_1 \cdot X$$ {#eq-line}

In which we use the data to discover the best values of the constants
$\beta_0$ and $\beta_1$.

However, this may not be the best functional form for this predictive
equation. For example, maybe an exponential or a logarithmic
relationship may be better:

$$\hat{y}_{23} \approx \beta_2 +\beta_3 \cdot e^X$$ {#eq-exp}

or

$$\hat{y}_{23} \approx \beta_4 +\beta_5 \cdot log(X)$$ {#eq-log}

or maybe

$$log(\hat{y}_{23}) \approx \beta_6 +\beta_7 \cdot log(X)$$ {#eq-loglog}

We can certainly investigate these, and hundreds of other possibilities
by building multiple models and comparing the performance between them.

Alternative Conditional Expectations [@breiman1985estimating] is a
technology that will automatically transform both the dependent and the
independent variables such that the relationship between the dependent
variable and a linear combination of the independent variables is as
straight as possible. Accordingly, building and testing hundreds of
linear regression models can be avoided. This is different from the
[Box-Cox transform](https://en.wikipedia.org/wiki/Power_transform), @box1964analysis
where the aim is to make the population more normal.

Although, as shown above, we can use linear regression to find
non-linear relationships between variables, some relationships may be
challenging, such as:

$$\hat{y}\approx\frac{\beta_0+\beta_1x+\beta_2x^2}{\beta_3+\beta_4x}$$ {#eq-ration}

Rational fractions like this may require full-blown non-linear
regression techniques which will require us to specify the form of the
equation and then use the data to find the values of the constants.

A very interesting recent development [@liu2024kan] in this space is the
Kolmogorov-Arnold Network, in which the activation functions sit on the
edges instead of the nodes. Like the weights they replace in a regular
conventional neural network, each one is different, and therefore by plotting the shapes of
these activation functions, one could, if the network is small enough,
intuit what is happening to the data as it goes through the network.
Although this provides *explainability*, it does not serve up an
equation.

The promise of symbolic regression is to find the optimum equation relating the independent variables to the dependent variable, which may *not* be linear, *and* the optimum values of the
equation constants in the same workflow.

### Typical Workflow

1.  The first step is deciding *what* we are going to encode so that the
    generated equations have a reasonable chance of modeling the data we
    present to the algorithm. Our equations will consist of operators,
    variables and constants, and here we are talking about operators.
    One may be tempted to simply include all operators known to man, but
    the more operators we use, the slower most algorithms run.

2.  The second step is to decide *how* we are going to measure the
    fitness of a particular equation. The standard workflow is to decode
    the equation into a function, apply that function to each row in the
    training dataset, and then calculate the mean squared error between
    the target value and the equation result.

3.  The third step is to decide *when* we will stop the workflow.
    Usually we will set a maximum number of generations *and* an error
    threshold that the best equation has to meet to stop the search.
    This equation is the one that will be offered-up as the answer at
    the end of the workflow.

4.  The fourth step is to generate an initial population of candidate
    equations. The quantity is usually in the thousands, and involves
    the random selection of the components mentioned in the first step.

5.  After the first fitness evaluation, the quality of the equations are
    usually quite poor. The best are selected, and then changed randomly
    to form the next generation, in a process influenced by biological
    evolution theory.

6.  We then perform the fitness evaluation on the new generation. We are
    expecting, of course, that each generation will produce better
    equations than the last, *on average*.

7.  Next, we check that the fitness value of the best equation and/or
    the number of generations tells us to stop. If not..

8.  Then we again select the best and change them for the next
    generation.

### Toy Dataset

We are going to build a toy dataset to demonstrate the workflow. In
addition to its small size (one predictor variable, 21 rows) the fact
that we know *exactly* what the generating equation is gives us a
yardstick to compare our results against. Here is the equation for
predicting API gravity from specific gravity:

$$\gamma_{API}=\frac{141.5}{\gamma_{o}}-131.5$$ {#eq-API}

| Symbol         | Meaning          | Units      |
|----------------|------------------|------------|
| $\gamma_{API}$ | API Gravity      | \[degAPI\] |
| $\gamma_o$     | specific gravity | \[1/wtr\]  |

: Toy Variables {#tbl-API .striped .hover}

And here is our dataset:

```{r}
#| label: fig-toy
#| fig-cap: Pre-Fit
# Generate Dataset
x<-seq(from=0.55,to=1.075,by=0.025)
y<-141.5/x-131.5
# Draw Graph
plot(x,y,main="Toy Dataset",
    xlab="Specific Gravity",
    ylab="API Gravity")
grid()
```

### Software Implementation

Most libraries available in Python for Symbolic Regression follow the
`Scikit-Learn` grammar model. We will use the `PySR` library
[@cranmer2023interpretable] for demonstration purposes. First, we import
the main function for the library:

`from pysr import PySRRegressor()`

Next, we call the main function to set-up the model. Here, we are using
default parameters for everything:

`myMod=PySRRegressor()`

Following that, we instruct the software to search for a suitable
equation for the data we have provided:

`myMod.fit(x.reshape(-1, 1),y)`

Here, `x` is specific gravity and `y` is the API gravity. The `reshape`
is necessary as `Scikit-Learn` expects more than one predictor variable.

Next, we can check the quality of the fit by plotting the predicted
values against the original values:

`y_pred=myMod.predict(x.reshape(-1, 1))`

The reader will notice that we have neglected to divide our dataset into
`test` and `train` buckets for this demonstration.

```{r}
#| label: fig-toy-fit
#| fig-cap: Clean Fit
# Mimic Results from synthDataSets.ipynb
y_fit<-141.5/x-131.500000554
y_raw<-141.5/x-131.5
# Draw Raw and Fit together
plot(x,y_raw,
    main="Toy Dataset Fit",
    xlab="Specific Gravity",
    ylab="API Gravity")
lines(x,y_fit,
    type="l",
    lty=1,
    col="red")
legend("topright",
    legend=c("Original","Predicted"),
    col=c("black","red"),
    lty=c(1,1),
    pch=c(1,1))
grid()
```

Next, if we like what we see, we can ask to inspect the final equation:

`myEq=myMod.sympy()`

And the evolutionary algorithm will typically present an output that
looks like this:

$$x_0-(x_0+0.013196754)+1.0131962+ \frac{x_0 (-132.5)- -141.5}{x_0}$$ {#eq-toy-raw}

This looks bloated, but we put it on a diet using a symbolic mathematics
 library (or high-school algebra):

`sym.simplify(myEq)`

which produces a much slimmer:

$$-131.500000554+\frac{141.5}{x_0}$$ {#eq-toy-fit}

Python is using $x_0$ to refer to the first dependent variable, which in this case is just $\gamma_o$.

### Noisy Toy

One of the challenges of *real* data is that it is noisy. Accordingly we
will add some noise to our toy dataset:

`w=(random.rand(21)-0.5)*2.5`

and re-run it to see whether symbolic regression is robust enough to
handle noise:

![Noisy Fit](/noisy_pysr_files/figure-html/cell-7-output-1.png){#fig-toy-noise}

This uniformly distributed noise was enough to throw-off the estimation of the
constants a bit, but the form of the equation was preserved:

$$-132.05688+\frac{141.88547}{x_0}$$ {#eq-toy-noise}

### Belly of the Beast

There are a large number of hyper parameters in a typical symbolic
regression model, and we defaulted all in the Toy dataset model above.

Here are a *few* of them:

#### Step 1 (Encoding)

`binary_operators=["+","-","*","/"]`

Here we choose which binary operators may appear in our equations. These
operators work on two numbers, for example $(a+b)$. On first glance, this
may appear to be restrictive, but for example, the ternary operation
$(a+b+c)$ can be chained as two binaries $(a+(b+c))$.

A more serious challenge is handling underflow and overflow. In
particular, the classic "division-by-zero" error appear very easily when
we are randomly assembling equations using operators, variables and
constants. Every library will use a different coping mechanism, but
common strategies include pre-assigning a value such as zero, one or
positive infinity to a calculation that will generate errors.

There are additional binary operators such as the power function $(a^b)$
or the maximum and minimum functions.

`unary_operators=None`

Here we choose the unary operators that may appear in our equations.
Some libraries call these "functions" instead. Examples include sine,
cosine and square root, the latter capable of being represented by a
power function mentioned above.

`maxsize=30`

This is the maximum length of a generated equation. Our (true) toy
equation has a length of five:

| 1     | 2      | 3     | 4                | 5     |
|-------|--------|-------|------------------|-------|
| minus | divide | 141.5 | specific gravity | 131.5 |

: Toy Equation Length {#tbl-eqn-len .striped .hover}

Notice that the generated toy equation @eq-toy-raw is significantly longer
than this, and only shrinks to 5 after post-processing. Accordingly, we
want to be liberal with this number.

As a side note, this way of writing an equation is called "Polish
Notation"[^1].

`maxdepth=None`

This is the maximum depth of a generated equation. Our toy equation has
a depth of three: 

![Toy Equation Tree](API_tree.png){#fig-tree-toy}

`None` in this case means that there are no restrictions on the depth of
the equation.

#### Step 2 (Fitness)

`elemtwise_loss="L2DistLoss()"`

This switch says that we will use least squares to judge the accuracy
of an equation. There are several canned loss functions like this, for
example `L1DistLoss` will use the average absolute difference between
the equation predictions and the actual values. In addition, one may
define custom loss functions using Julia expressions in `PySR`.

Here is our Mean Squared Error (`L2DistLoss`):
$$ \frac{1}{n} \sum_{k=1}^{n}{\left(\hat{y}_k-y_k \right)}^2$$ {#eq-MSE}
and our Mean Absolute Error (`L1DistLoss`):
$$ \frac{1}{n} \sum_{k=1}^{n}{\left|\hat{y}_k-y_k \right|}$$ {#eq-MAE}
The latter is thought to be less sensitive to outliers. For both these equations:

|Symbol|Description|
|---|---|
|$\hat{y}_k$|$k^{th}$ predicted value|
|$y_k$|$k^{th}$ measured value|
|$n$|total number of values|

: Loss Function Terminology {#tbl-err-fun .striped .hover}

`parsimony=0.0`

This is a regularization weight. Unlike Lasso or Ridge regression, the
length of the equation is penalized instead of the size of the
coefficients. Other libraries consider the length of the equation and
over-simplified measure of complexity and propose other measures.

The idea here is that a simple equation that is very nearly as accurate
as a complex one is more "explainable", and therefore there needs to be
a mechanism to encourage simpler equations.

`model_selection="best"`

This switch says that the models selected will be a specified trade-off
between accuracy and complexity. There are other formulations such as
`score` involving log-loss derivatives or one could simply choose
`accuracy`.

`should_simplify=True`

This switch says that occasionally equations generated during the search
will be simplified. Superficially, it may appear to be a good idea to
simplify the equations after *every* generation, to cut down on "bloat"
or equations that are more complex than they need to be. For example,
$(x_0+5x_0-2x_0)$ vs. just $(4x_0)$.

However, the `PySR` developers found that allowing some bloat improved
the search significantly.

`Tournament_selection_n=15`

In a nod to evolutionary biology, this says that models compete in
groups of 15 for selection. The larger this number, the fewer "winners"
there are after every round.

#### Step 3 (Stopping)

`niterations=100`

This is equivalent to an epoch in other machine learning algorithms. 100
is actually quite low. Other libraries require low thousands of epochs
to produce the same result.

`max_evals=None`

This switch counts the number of equation evaluations performed, and
stops the search after a specified number. Equation evaluations are a
significant fraction of the run time, especially as the equations get
longer and contain more
[transcendental](https://en.wikipedia.org/wiki/Transcendental_function) functions.

`timeout_in_seconds=None`

This switch keeps track of the run time and stops the search after a
specified number.

#### Step 4 (Population)

`populations=31`

This switch says that we are going to run 31 populations in parallel.
This suggests that at the end of the run we should have 31 *almost*
independent "best" solutions to choose from.

In practice, however, some populations "go extinct" which in our context
means that they fail to produce reasonable equations. `PySR` does allow
some "immigration" between populations, which produces benefits in both
directions. The "better" populations have some diversity injected into
them, and the "worse" populations are given new opportunities to
succeed.

`population_size=27`

This switch says that each population will have 27 individuals. Notice
that the tournament size is smaller than this, so that there is more
than one competition between individual equations.

### Quiet Toy

The `PySR` library is relatively silent over control of constants. In its implementation, the equation forms are generated first, which for our toy example would be:

$$ \frac{a_{00}}{x}+a_{01}$$ {#eq-toy-blank}

and randomly generated numbers used to fill the constants ($a_{00},a_{01}$). Then in a second step, the values of these constants are tuned to minimize errors.  In more theoretically rigorous implementations of the evolutionary algorithm, such as `gramEvol`, (@noorian2016gramevol) these constants, like the operators, have to be specified up front, and are selected based on fitness.

For our toy problem, we can supply a list of constants, of which only two are correct like this:

`w<-seq(from=121.5,to=161.5, by=10)`

and then the set-up for the problem would look like this:

```{r}
#| label: tbl-backus
#| tbl-cap: Backus-Naur Description of Toy Problem
# Import Library
library("gramEvol")
# List of constants
w<-seq(from=121.5,to=161.5, by=10)
# Define the rules
ruleDef <- list(expr = grule(op(expr, expr), var,con),
                op = grule('+', '-', '*','/'),
                var = grule(x),
                con = gvrule(w))
# Create the Grammar
grammarDef <- CreateGrammar(ruleDef)
# Print Backus-Naur version
grammarDef
```

Then we can supply other important hyper parameters (e.g. fitness function, number of iterations, complexity limits) and `gramEvol` will find our function after about 1500 generations:
```{r}
#| label: tbl-gramEvol
#| tbl-cap: gramEvol Results
# Fitness Function
SymRegFitFunc <- function(expr) {
  result <- eval(expr)
  if (any(is.nan(result)))
    return(Inf)
  return (mean(log(1 + abs(y - result))))
}
# Random Number Generator
set.seed(13)
# Call to main function
ge <- GrammaticalEvolution(grammarDef,
                           SymRegFitFunc,
                           terminationCost = 0.1,
                           iterations = 2500,
                           max.depth = 10)
ge
```

This approach is clearly useful when our constants can only assume specific values, for example the freezing point of water at atmospheric pressure ($273.15 K$)  but less useful when we don't know enough to define all our relevant constants ahead of time.

@virgolin2022coefficient present two algorithms that adjust the value of the constants as an integral part of the evolutionary process, but conclude that "coefficient mutation does not always make a difference". This is disappointing.

### Alternatives to Evolution

Although genetic programming has proven to be most popular approach to
Symbolic Regression [@dong2025recent], it has been criticized for being
slow and producing bloated equations.

Deterministic methods such as brute-force search [@udrescu2020ai],
mathematical programming [@austel2017globally] and sparse regression
[@muthyala2025symantic] have been employed. The latter is particularly
good at removing variables that don't materially contribute to the
prediction. Typically, they don't solve the speed problem, but are
thought to produce more easily interpreted equations.

Methods which rely on information technology have also been used,
especially of late. The idea here is to replace the random changes to
the equations that are the heart of the evolutionary approach with
changes based on what we know about th data. Sometimes this information
is learned from the data [@anthes2025transformer], and other times it is
supplied externally [@keren2023computational] as "human experience". In
the latter case, there is the risk that the search space may become so
restricted that nothing new can be learned.

Neural Network methods have also made an appearance. An early approach
[@martius2016extrapolation] replaced the activation functions in an ANN
with trancendental functions such as sine and log. The main idea here is
that each layer would host multiple kinds of functions instead of just
one. The more recent approaches [@kamienny2022end] pre-train a
transformer model to learn the relationships between data and equations
using synthetic data. The major advantage here is that if we ignore the
pre-training phase, the training phase may be quick enough to use the
model in real time.

### Next Steps

1.  Examine our dataset and describe some of the equations that have
    been used historically to calculate molecular mass.

2.  Compare the equations that are generated by `PySR` for
    our dataset as the options are varied.
    
3.  Compare the equations that are generated by evolutionary algorithms vs. deterministic algorithms.


[^1]: A post-fix variant was used on scientific calculators in
the 1970's as it was more efficient. The learning curve, however, was
brutal for mere mortals, so that owning one of these 'RPN' 
[artifacts](https://en.wikipedia.org/wiki/HP-35) became a status symbol 
in the engineering community well into the 1990's.

## Analysis and Results

### Data Exploration and Visualization

Let us take a first look at the dataset from Goossens
[@goossens1996prediction]:

```{r}
# Read Goosens Dataset from flat file
# Positron
#df<-read.csv("./c694/goossens_raw.csv")
# Quarto
df<-read.csv("./goossens_raw.csv")
```

We have 3 variables:

| Variable | Description        | Designation |
|----------|--------------------|-------------|
| $Mw$     | Molecular Mass     | dependent   |
| $SG$     | Specific Gravity   | independent |
| $TBP$    | True Boiling Point | independent |

: Goossens Variables {#tbl-goo-var .striped .hover}

and 70 rows. 

*Molecular Mass* is a proxy for the size of the molecule.  For those of you who remember your high school chemistry, it is the mass of the substance per unit mole. *Specific Gravity* is the density of the substance divided by the density of water. Again, for those of you who remember your high school physics, density is mass per unit volume, so we are expecting these two to be related somehow.  *True Boiling Point* is the temperature at which the substance boils at atmospheric pressure.  The true boiling point of water is 100 C.

This is a very small dataset by modern machine learning
standards. However, due to the cost of acquiring the molecular mass
data, this dataset will be considered "large" by chemical engineering
standards.

One of the questions we want to answer is whether datasets like this are
"too small" for symbolic regression.

#### Univariate Analysis
Here is a summary of the dataset:

```{r}
#| label: tbl-goo-sum
#| tbl-cap: Goosens Dataset Summary
# Summarize Goossens Dataset
print(summary(df))
```

The reader will notice that the specific gravity value range is small compared to the boiling point and molecular mass ranges.  Also, the mean of the molecular mass data is nowhere near its median.  Lets take a closer look:
```{r}
#| label: fig-his_mw
#| fig-cap: Raw Molecular Mass Histogram
hist(df$MW,
     main="",
     xlab="Molecular Mass")
```

The reader will notice a strongly right-skewed distribution with an empty "bucket" near the top.  Lets take a peek at the box plots:

```{r}
#| label: fig-box_mw
#| fig-cap: Raw Molecular Mass Box-and-Whiskers Plot
boxplot(df$MW,
        horizontal=TRUE)
```

The reader will notice that all the values greater than about 600 would be considered outliers if the molecular masses were normally distributed.

Turning to the True Boiling Point variable:

```{r}
#| label: fig-box_tbp
#| fig-cap: Raw Boiling Point Box-and-Whiskers Plot
boxplot(df$TBP,
        horizontal=TRUE,
        col="lightblue")
```

We appear to have no outliers for this variable, but this variable is not normally distributed either.  Lets check:

```{r}
#| label: fig-his_tbp
#| fig-cap: Raw Boiling Point Histogram
hist(df$TBP,
     main="",
     xlab="True Boiling Point",
     col="lightblue")
```

We appear to have a bimodal distribution here.  Lets compare this with the other predictor, specific gravity:

```{r}
#| label: fig-his_sgo
#| fig-cap: Raw Specific Gravity Histogram
hist(df$SG,
     main="",
     xlab="Specific Gravity",
     col="lightgreen")
```

This distribution is skewed to the right, but not as strongly as molecular mass.  No signs of a bimodal distribution here.  Here is the box plot:

```{r}
#| label: fig-box_sg
#| fig-cap: Raw Specific Gravity Box-and-Whiskers Plot
boxplot(df$SG,
        horizontal=TRUE,
        col="lightgreen")
```

It is interesting that the largest specific gravity values appear to be outliers, even though this variable appears to be the "most normally distributed" of the three.

#### Bivariate Analysis
Here is a plot of molecular mass vs. specific gravity:

```{r}
#| label: fig-goo-mwXsg
#| fig-cap: Molecular Mass vs Specific Gravity Goossens Data
plot(df$SG,df$MW,
    main="Goossens Dataset",
    xlab="Specific Gravity",
    ylab="Apparent Molecular Mass")
grid()
```

Although there appears to be a clear linear relationship between
molecular mass and specific gravity at low gravity numbers, the
heteroscedasticity explodes above a gravity of about 0.75.

Notice that the molecular mass "outliers" appear to be different from the specific gravity "outliers".  A reasonable assumption going in was for these two variables to be (more) positively correlated, placing both "outlier" groups in the upper right corner of the plot.

Notice that the largest variation in molecular mass corresponds to the specific gravity range 0.8-0.9, which, according to @fig-his_sgo is our mode.

Here is the molecular mass vs. true boiling point scatter plot:

```{r}
#| label: fig-goo-mwXtb
#| fig-cap: Molecular Mass vs Boiling Point Goossens Data
plot(df$TBP,df$MW,
    main="Goossens Dataset",
    xlab="True Boiling Point",
    ylab="Apparent Molecular Mass")
grid()
```

There seems to be a monotonically increasing relationship between molecular mass and true boiling point, which matches the expectation going in, unlike @fig-goo-mwXsg.  There is a possible "pole" around the boiling point of 1000, which we will discuss in our results.

At this point, it may be tempting to ignore the effect of specific gravity on the prediction of molecular mass.

Here we plot the two independent variables against one another:

```{r}
#| label: fig-goo-sgXtb
#| fig-cap: Specific Gravity vs Boiling Point Goossens Data
plot(df$TBP,df$SG,
    main="Goossens Dataset",
    xlab="True Boiling Point",
    ylab="Specific Gravity")
grid()
```

This plot suggests that the correlation is poor between true boiling
point and specific gravity. Presumably, the specific gravity helps to
reduce the scatter around the trend of molecular weight vs. true boiling
point.

There is a second, smaller independent dataset available publicly
[@hosseinifar2021predictive] which we can use for verification. Let us
compare it to the Goossens dataset:

```{r}
#| label: fig-hoss-mwXtb
#| fig-cap: Molecular Mass vs Boiling Point. Both Datasets
# Read Hosseinifar Dataset from flat file
# Positron
#dfh<-read.csv("./c694/hosseinifar_raw.csv")
# Quarto
dfh<-read.csv("./hosseinifar_raw.csv")
# Plot Goossens and Hosseinifar together
plot(df$TBP,df$MW,
     main="Goossens vs. Hosseinifar",
     xlab="True Boiling Point",
     ylab="Molecular Mass",
     col="blue")
points(dfh$TBP,dfh$MW,
       col="red",
       pch=2)
legend("topleft",
       legend=c("Goossens","Hosseinifar"),
       col=c("blue","red"),
       pch=c(1,2))
grid()
```

```{r}
#| label: fig-hoss-mwXsg
#| fig-cap: Molecular Mass vs Specific Gravity. Both Datasets
# Plot Goossens and Hosseinifar together
plot(df$SG,df$MW,
     main="Goossens vs. Hosseinifar",
     xlab="Specific Gravity",
     ylab="Molecular Mass",
     col="blue")
points(dfh$SG,dfh$MW,
       col="red",
       pch=2)
legend("topleft",
       legend=c("Goossens","Hosseinifar"),
       col=c("blue","red"),
       pch=c(1,2))
grid()
```

The two datasets appear to be compatible, even though the variation of
the Hosseinifar dataset is significantly smaller.

#### Existing Correlations

The existing correlations available for estimating molecular mass from
true boiling point and specific gravity provide hints as to what kind of
equations we expect to see from the algorithm. Many of them predate
symbolic regression and therefore were developed by people using
intuition and experimentation.

Here are a few of them using the following nomenclature:

| Symbol           | Meaning                                           |
|------------------|---------------------------------------------------|
| $M_w$            | Apparent Molecular Mass                           |
| $T_b$            | True Boiling Point Temperature                    |
| $\gamma_o$       | Specific Gravity                                  |
| $a_{00}..a_{09}$ | Empirical Constants                               |
| $K_w$            | Characterization Factor (intermediate value)      |
| $X_0...X_3$      | Intermediate Variables                            |

: Correlation Nomenclature {#tbl-cor-nom .striped .hover}

##### Hariu & Sage (1969)

$$
M_w = a_{00} + a_{01} K_w + a_{02} K_w^2 + a_{03} T_b K_w + a_04 T_b K_w^2 + a_{05} T_b^2 K_w + a_{06} T_b^2 K_w^2
$$ {#eq-harlu}

$$K_w =\frac{\sqrt[3]T_b}{\gamma_o}$$ {#eq-UOP}

##### Kesler & Lee (1976)

$$M_w = X_0 + \frac{X_1}{T_b} + \frac{X_2}{T_b^2}$$ {#eq-kesler}

$$X_0 = a_{00} + a_{01} γ_o+ \left (a_{02} + a_{03} γ_o \right ) T_b$$ {#eq-kes-0}

$$
X_1 = \left (1+ a_{04} γ_o + a_{05}γ_o^2 \right ) \left (a_{06} + \frac{a_ {07}}{T_b} \right ) \cdot 10^7
$$ {#eq-kes-1}

$$
X_2 = \left (1+ a_{08} γ_o+ a_{09} γ_o^2 \right ) \left (a_{10} + \frac{a_{11}}{T_b} \right ) \cdot 10^{12}
$$ {#eq-kes-2}

##### American Petroleum Institute (1977)

$$
M_w = a_{00} e^ {\left (a_{01} T_b \right )} e^{\left (a_{02} γ_o \right )} T_b^{a_{03}} γ_o^ {a_{04}}
$$ {#eq-api-77}

##### Winn, Sim & Daubert (1980)

$$M_w = a_{00} T_b^{a_ {01}} γ_o^{a_{02}}$$ {#eq-win}

##### Riazi & Daubert (1980)

$$M_w = a_{00} T_b^{a_ {01}}γ_o^{a_{02}}$$ {#eq-riazi-80}

##### Rao & Bardon (1985)

$$ln {M_w} = (a_{00} + a_{01} K_w) ln (\frac{T_b} {a_{02} + a_{03} K_w} )$$ {#eq-rao}

See @eq-UOP

##### Riazi & Daubert (1987)

$$
M_w = a_{00} T_b^{a_{01}} γ_o^{a_{02}} e^{\left (a_{03} T_b + a_{04} γ_o + a_{05} T_b γ \right )}
$$ {#eq-riazi-87}

##### Goossens (1996)

$$M_w = a_{00} T_b^{X_0}$$ {#eq-goossens}

$$
X_0 =\frac {a_{03} + a_{04} ln {\left (\frac{T_b}  {a_{05} - T_b} \right )}}  {a_{01} γ_o + a_{02}}
$$ {#eq-goo-0}

##### Linan (2011)

$$
M_w = a_{00} e^{\left (a_{01} T_b \right )} e^{\left (a_{02} γ_o \right )} T_b^ {a_{03}} γ_o^{a_{04}}
$$ {#eq-linan}

##### Hosseinifar & Shahverdi (2021)

$$M_w = {\left [a_{00} T_b^{a_{01}} {\left (\frac{3+2γ_o} {3-γ_o} \right )}^{\frac{a_{02}}{2}} + a_{03} T_b^{a_{04}} {\left (\frac{3+2γ_o}{3-γ_o} \right )}^{\frac{a_{05}}{2}} \right ]}^{a_{06}}$$ {#eq-hosseinifar}

##### Stratiev (2023)

$$
M_w = a_{00} + a_{01} e^{\left [a_{02} e^{\left (a_{03} \frac{T_b^{a_{06}}}{γ_o^{a_{05}}} \right )} \right ]}
$$ {#eq-stratiev}

The reader will notice that all the correlations are non-linear, and
that only a few of them can be easily transformed into a linear
relationship. Some of the additional operators we may want to consider from an inspection of these equations
include:

| Operator | Type   | Description                                   |
|----------|--------|-----------------------------------------------|
| pow      | binary | one expression raised to the power of another |
| log      | unary  | logarithm of an expression                    |
| exp      | unary  | [antilogarithm](https://mathworld.wolfram.com/Antilogarithm.html) of an expression|
| sqr      | unary  | expression squared                            |
| cub      | unary  | expression cubed                              |
| inv      | unary  | inverse of an expression                      |

: Partial List of Additional Operators {#tbl-ope-add .striped .hover}

### Modeling and Results

In this sub-section, we will apply a series of symbolic regression models to our Goossens dataset, and look at the structure and performance of each generated equation.

#### Default Run

Our first experiment is to run `PySR` with default parameters against
our molecular mass dataset. The resulting equation looks nothing like
the existing equations:

$$
M_w=a_{00}+\frac{a_{01}}{\gamma_o-a_{02}}+\frac{T_b\cdot (a_{03}\cdot T_b-a_{04})}{\gamma_o\cdot (a_{05}-\frac{a_{06}}{T_b})}
$$ {#eq-res-def}

But its performance is quite impressive considering its restricted
grammar of addition, subtraction, multiplication and division:

![Default Parameter Run](moleDataSet_files/figure-html/cell-11-output-1.png){#fig-res-def}

It even has a (slightly) better correlation coefficient with the raw
molecular mass data than @eq-goossens presented with this data in the
Goossens paper:

+----------+----------+----------+----------+-----------+----------+
|          | Raw SG   | Raw TBP  | Raw MW   | Goossens\ | This\    |
|          |          |          |          | Equation  | Equation |
+:=========+=========:+=========:+=========:+==========:+=========:+
| Raw SG   | 1.000000 | 0.625218 | 0.334852 | 0.339126  | 0.337190 |
+----------+----------+----------+----------+-----------+----------+
| Raw TBP  | 0.625218 | 1.000000 | 0.869591 | 0.868486  | 0.871037 |
+----------+----------+----------+----------+-----------+----------+
| Raw MW   | 0.334852 | 0.869591 | 1.000000 | 0.999711  | 0.999847 |
+----------+----------+----------+----------+-----------+----------+
| Goossens | 0.339126 | 0.868486 | 0.999711 | 1.000000  | 0.999798 |
| Equation |          |          |          |           |          |
+----------+----------+----------+----------+-----------+----------+
| This     | 0.337190 | 0.871037 | 0.999847 | 0.999798  | 1.000000 |
| Equation |          |          |          |           |          |
+----------+----------+----------+----------+-----------+----------+

: Default Run Correlation Coefficients {#tbl-res-def .striped .hover}

There are some challenges. The constant $a_{02}$ is only slightly larger
than the largest $\gamma_o$ (specific gravity) in the dataset. This
limits the extrapolation power of this equation to higher specific
gravities than seen in this dataset. Extrapolation is one of the
strengths of Symbolic Regression [@sahoo2018learning]

Next, we look at the residuals, plotted against the raw molecular mass:

![Default Run Residuals](moleDataSet_files/figure-html/cell-17-output-1.png){#fig-res-def-res}

Visually, the residuals *appear* to be normally distributed, but running the Shapiro-Wilk test reveals a probability of only 0.001 of the residuals being normally distributed.  So much for eye-balling!

Let us see how *bad* it is:
```{r}
#| label: fig-defrun-resid
#| fig-cap: Q-Q Plot for the Default Run Residuals
# Read Goossens Default Run results from flat file
# Positron
#df<-read.csv("./c694/default_output.csv")
# Quarto
dfd<-read.csv("./default_output.csv")
# Generate QQPlot
qqnorm(dfd$Fit_Resid)
qqline(dfd$Fit_Resid, col="red")
grid()
```

The plot suggests that small residuals (say $\pm 2.5$) are normally distributed, but the large residuals are not.

Out of mere curiosity, let us examine the residuals from the Goossens correlation (@eq-goossens):

![Goossens Correlation Residuals](moleDataSet_files/figure-html/cell-19-output-1.png){#fig-res-def-goo}

This looks largely similar to the default-run residuals, until it is noticed that the zero line does not bisect the plot.  The Shapiro-Wilk test p-value is zero to *six* significant figures.

#### Power Run

Our Second experiment is to drop the "division" binary operator
$\left (\frac{a}{b} \right )$ and add the "power" binary operator
$\left (a^b \right )$ as the latter is a popular component of the
existing correlations.

There is noticeable drop in the quality of the match visually:

![Power Operator Run](around2_files/figure-html/cell-6-output-1.png){#fig-res-pow}

Although the equation has got simpler:

$$M_w=a_{00} \cdot a_{01}^{\gamma_o^{-a_{02}} + a_{03} \cdot T_b} + a_{04}$$ {#eq-res-pow}

and the correlation coefficient has degraded slightly:


+-----------+-----------+-----------+-----------+-----------+
|           | Raw SG    | Raw TBP   | Raw MW    | This      |
|           |           |           |           | Equation  |
+:==========+==========:+==========:+==========:+==========:+
| Raw SG    | 1.000000  | 0.625218  | 0.334852  | 0.325151  |
+-----------+-----------+-----------+-----------+-----------+
| Raw TBP   | 0.625218  | 1.000000  | 0.869591  | 0.868747  |
+-----------+-----------+-----------+-----------+-----------+
| Raw MW    | 0.334852  | 0.869591  | 1.000000  | 0.997281  |
+-----------+-----------+-----------+-----------+-----------+
| This      | 0.325151  | 0.868747  | 0.997281  | 1.000000  |
| Equation  |           |           |           |           |
+-----------+-----------+-----------+-----------+-----------+

: Power Operator Run Correlation Coefficients {#tbl-res-pow .striped .hover}

One can argue that nested exponents $\left (a^{b^c} \right )$ aren't
very explainable. However the Stratiev correlation (@eq-stratiev) does this with
Euler's number $\left ( a \cdot e^{b \cdot e^{c}} \right )$. In the
hydrocarbon flow literature, nested logarithms $ln(ln(a))$ are more
common.

#### Exponential Run

Our third experiment was to replace the binary power operator with unary
logarithm and exponential operators. Our initial results were a major
surprise as neither transcendental operator made it to the final
equation:

$$ M_w= - T_b \cdot \left(a_{00} \cdot T_b - a_{01}\right) \left(a_{02} \cdot 10^{-6} \left(a_{03} \cdot \gamma_o - 2 \cdot T_b \right) \left(T_b - a_{04}\right) - 1\right) + a_{05}$$ {#eq-res-exp-1}

Accordingly, we re-set the random number generator and tried again:

$$ M_w= a_{00}\cdot T_b + a_{01} \cdot e^{- \gamma_o^{2} + a_{02}\cdot \gamma_o + a_{03} \cdot T_b}$$ {#eq-res-exp-2}

This is a dramatically different equation. This experience suggests that
the *real* reason the deterministic algorithms are still under active
development today is to avoid this kind of ambiguity.

Our correlation coefficients are *still* not as good as the default run:

+----------+----------+----------+----------+----------+----------+
|          | Raw SG   | Raw TBP  | Raw MW   | 1st\     | 2nd\     |
|          |          |          |          | Equation | Equation |
+:=========+=========:+=========:+=========:+=========:+=========:+
| Raw SG   | 1.000000 | 0.625218 | 0.334852 | 0.348819 | 0.344733 |
+----------+----------+----------+----------+----------+----------+
| Raw TBP  | 0.625218 | 1.000000 | 0.869591 | 0.869577 | 0.867574 |
+----------+----------+----------+----------+----------+----------+
| Raw MW   | 0.334852 | 0.869591 | 1.000000 | 0.997705 | 0.998420 |
+----------+----------+----------+----------+----------+----------+
| First    | 0.348819 | 0.869577 | 0.997705 | 1.000000 | 0.999497 |
| Equation |          |          |          |          |          |
+----------+----------+----------+----------+----------+----------+
| Second   | 0.344733 | 0.867574 | 0.998420 | 0.999497 | 1.000000 |
| Equation |          |          |          |          |          |
+----------+----------+----------+----------+----------+----------+

: Exponential Run Correlation Coefficients {#tbl-res-exp .striped .hover}

*But* the risk of "division by zero" errors during extrapolation are
negligible.

Also notable in this run is the absence of the logarithm function to
linearize the relationship between these two variables:

![Raw Data Scale Change](around3_files/figure-html/cell-14-output-1.png){#fig-res-hum}

This "obvious" human observation appears not to be an optimum
transformation, or was eliminated by the evolutionary algorithm before
it could reach its full potential.

#### Aeon Run

The fourth experiment is the same as the third experiment, except that
we run it for ten times as long. Here, we are interested in whether
different starting points will eventually converge to similar equations.

Here is the first equation after a thousand iterations:

$$
M_w=(a_{00}\cdot T_b-a_{01})e^{a_{02}\cdot 10^{-9}T_b^2(a_{03}\cdot \gamma_0 +a_{04}\cdot T_b)}
$$ {#eq-res-eon-1}

And the second equation[^2]:

$$
M_w=-a_{00}\cdot \gamma_o+\frac{e^{-\gamma_o^2+{log(T_b)}^3}}{T_b^{a_{01}}} +a_{02}\cdot T_b
$$ {#eq-res-eon-2}

The correlation coefficients, however, now rival the default run:

|                 |   Raw Mw | First Equation | Second Equation |
|:----------------|---------:|---------------:|----------------:|
| Raw Mw          | 1.000000 |       0.998880 |        0.999324 |
| First Equation  | 0.998880 |       1.000000 |        0.998851 |
| Second Equation | 0.999324 |       0.998851 |        1.000000 |

: Aeon Run Correlation Coefficients {#tbl-res-eon .striped .hover}

Both equations have evolved significantly by adding 900 iterations, but "similar,
they are not" as [everyone's favourite 900 year old](https://en.wikipedia.org/wiki/Yoda)
would say.

The second equation is truly fascinating for two reasons. Neither the
power operator $\left (a^b \right )$ nor the division operator
$\left (\frac{a}{b} \right)$ were provided to the algorithm for this
run, and yet we have found the middle expression
$\left( \frac{e^{-\gamma_o^2+{log(T_b)}^3}}{T_b^{a_{01}}} \right)$.

#### Box-Cox Run

The reader may remember that our molecular mass (@fig-his_mw) was *not* normally distributed.  Accordingly, here we transform our dependent variable to see if this improves the distribution of our residuals.  Our optimum exponent works out to be $-0.3624$ according to the python `stats` library, which is fairly close to zero.

Our transformed molecular mass distribution is hardly normal:

![transformed molecular mass histogram](around1_files/figure-html/cell-6-output-1.png){#fig-yraw-trans}

The other thing the reader may have noticed in @fig-yraw-trans is that the transformed variable range is much narrower.  If we round-up to an exponent of zero, then our transform will now be logarithmic:

![transformed molecular mass](box-cox_files/figure-html/cell-5-output-1.png){#fig-ytrans-log}

This expands the range of the dependent variable but probably makes it less Gaussian.  Let us go in the opposite direction and round-down to an inverse square root:

![transformed molecular mass](box-cox_files/figure-html/cell-4-output-1.png){#fig-ytrans-sqrt}

This appears to compress the range of the transformed variable even more. Let us stay with $\lambda=-0.3624$ and press on.  Using all four basic binary operators (like the default case) and adding exponentials and logarithms (like the exponential case), our fit looks quite good in Box-Cox space:

![transformed prediced vs. actual, Box-Cox Run](around1_files/figure-html/cell-8-output-1.png){#fig-fit-boxcox}

The residuals in Box-Cox space are more spread-out:

![transformed Residuals, Box-Cox Run](around1_files/figure-html/cell-11-output-1.png){#fig-resid-boxcox}

And the Shapiro-Wilk test calculates the probability of the residuals being normal to 0.0507, which although higher, is barely over a level of significance of 0.05.  When we untransform the predictions, and compare them to the measured molecular masses, the fit looks a little smoother:

![Untransformed prediced vs. actual, Box-Cox Run](around1_files/figure-html/cell-19-output-1.png){#fig-fit-unboxcox}

This run produced the same equation structure at 100, 500 and 1000 iterations, which was *not* the case for the `PySR` runs in the Aeon experiment above:

$$ M_w^t = \left (\gamma_o \left(a_{00} T_b -a_{01} \right) log{(\gamma_o)} + a_{02} \right) log{(T_b)} $$ {#eq-res-boxcox}

We do need to remember to reverse the Box-Cox transform:

$$ M_w = {\left(\lambda M_w^t +1 \right)}^{\frac{1}{\lambda}} $$ {#eq-unboxcox}

The correlation coefficient for the longest run (1000 iterations) is in line with the Aeon runs discussed earlier:

|               | Raw Mw  | This Equation |
|:--------------|--------:|--------------:|
| Raw Mw        | 1.000000|       0.995897|
| This Equation | 0.995897|       1.000000|

: Box-Cox Run Correlation Coefficients (Transform Reversed) {#tbl-res-boxcox .striped .hover}

A quick look at the residuals between the Reverse-Transformed predictions and the raw molecular masses tell the story:

![Residuals outside of Box-Cox Space, Box-Cox Run](around1_files/figure-html/cell-20-output-1.png){#fig-resid-unboxcox}

Residuals are small for molecular masses below about 350, but explode above 1000.  Statistically speaking, this may be good if we are more uncertain about the accuracy of the larger molecular masses, but the engineering point of view is different for exactly the same reason. Larger deviation from existing datapoints in regions where the datapoints are scarce is risky business.

#### First Sparse Run
It would seem remiss to not *at least* look at one of the deterministic models.  The first library to work without requiring long-depreciated versions of Python is `SyMANTIC` (@muthyala2025symantic) so we set up the run to be equivalent the the *Default Run* above.  Our "winning" equation is:

$$ M_w=-a_{00} \cdot \gamma_o +\frac{a_{01}\cdot \gamma_o}{T_b}+a_{02}\cdot T_b -a_{03}$$ {#eq-res-mth-1}

Compare this to @eq-res-eon-2. Similar, but much simpler.  The quality of the fit, however, is disappointing:


|               | Raw Mw  | This Equation |
|:--------------|--------:|--------------:|
| Raw Mw        | 1.000000|       0.970451|
| This Equation | 0.970451|       1.000000|

: First Sparse Run Correlation Coefficients {#tbl-res-mth-1 .striped .hover}

A pleasant surprise is that we will have no "division by zero" problems with this equation as $T_b$ is in Kelvin.

A sparse regression model, "under the hood" is just a linear regression model.  Accordingly, even though we are not thrilled with the fit, maybe the residuals are normal...

![1st Sparse Run Residuals](around7a_files/figure-html/cell-12-output-1.png){#fig-res-spa-res}

Never mind. Even if these residuals are normal (*they are not*) they are *definitely not* random. Moving right along...

#### Second Sparse Run
This leads to a second deterministic experiment, in which we add the logarithm and exponential functions, but leave the division operator in place. These is the same operator set used in the Box-Cox run above. 

Our new equation is *not* complex either:

$$ M_w=-a_{00}\cdot \gamma_o\cdot T_b +a_{01}\cdot T^2_b +a_{02}\cdot e^{\gamma_o} +a_{03}$$ {#eq-res-mth-2}

And our fit improves slightly:

| | Raw Mw |
|:---|---:|---:|
| Raw Mw|1.000000|
| 1st Run Equation | 0.970451 |
| 2nd Run Equation | 0.985800 |

: Second Sparse Run Correlation Coefficients {#tbl-res-mth-2 .striped .hover}

But our plot of measured molecular mass versus predicted molecular mass is not impressive at all:

![Second Sparse Run](around8_files/figure-html/cell-11-output-1.png){#fig-res-mth-2}

#### Validation Runs

As promised, we used the Hosseinifar dataset of 10 records (12.5%) to
validate some of these equations:

| Run | Correlation Coefficient|
|:---|---:|
| Raw Mw   | 1.000000 |
| Default | 0.999957 |
| Goossens Correlation| 0.999939 |
| Power | 0.996964 |
| Exponential #2 | 0.998954 |
| Aeon #1 | 0.999973 |
| Aeon #2 | 0.999921 |
| Box-Cox | 0.999696 |
| Sparse #1| 0.992303 |
| Sparse #2| 0.996331 |

: Validation Run Correlation Coefficients {#tbl-res-val .striped .hover}

All of these are very good, with the validation coefficients exceeding
the training coefficients. We will resist the temptation to declare
victory over overfitting, as the Hosseinifar dataset is far better
behaved than the Goossens dataset.

A better approach with the evolutionary model is probably to start increasing `parsimony` or decreasing `maxsize` or `maxdepth` until the correlation coefficients
start to deteriorate.

A different approach is warranted with the deterministic models.  It is possible that these are *underfit*, meaning that we can expand the number of unique terms the algorithm initially considers, or the number of terms we want to keep.

#### General Observations

The "UOP Characterization Factor" @eq-UOP which (at least) two
researchers decided was an important correlating variable for molecular
mass, was not discovered by our symbolic regression model.

In fairness, however, we dropped the division operator early in our
experimentation, and never asked the model to consider a "cube root"
unary operator.

Replication is very challenging with these symbolic regression libraries
that use evolutionary algorithms. Re-running the same dataset on the
same machine with the same libraries produces different equations. In
many `Scikit-Learn` algorithms, answer variation can be minimized by
seeding the random number generator the same way each time, but in
`PySR` we also have to turn off the optimizations that allow Julia to
run the calculations massively in parallel, which contributes to the
criticism that these algorithms are slow.

The deterministic symbolic regression libraries don't need random number seeds, and the answers stay the same between runs.  The equations are definitely more *explainable*, but appear to be less accurate.  Does this accuracy matter?  Is it real?  The answer to these questions depend on how good we think our measurements are!

In the engineering world, an old trick to speed-up calculations is to
use look-up tables instead of functions. The idea is that even though we
may know exactly what the governing equations are, producing look up
tables from these functions ahead of time and then applying Gaussian
interpolation in real time may be faster. This is a *very* similar
concept to the idea of pre-training a transformer network to do symbolic
regression.

This paper has shown that high fidelity models can be constructed from
"cheap" components (i.e. arithmetic operators). Maybe *explainability* is overrated, and functions carefully built using symbolic regression may be competitive with old-man Gauss!

If this turns out to be true, then symbolic regression will make the
full circle and return to it roots.

### Conclusions

In summary, we have explored Symbolic Regression using evolutionary and deterministic algorithms on our molecular mass dataset. We have not spent much time on different kinds of engines (e.g. neural networks, mathematical programming) or tested the extrapolation chops of this technology.

Our main conclusions are:

1.  Symbolic Regression can generate reasonable predictions when trained
    on datasets too small for neural networks.

2.  Evolutionary Symbolic Regression will typically generate novel equations from the human point of view, but these equations can be as accurate than the ones created by humans.

3.  The "explainability" of the generated equations are probably
    overrated, unless enough guardrails are put up to constrain the
    space searched by the evolutionary Symbolic Regression algorithm.

4.  The ability of Evolutionary Symbolic Regression to easily find alternative equations of about the same accuracy means that humans can repeat the workflow until an equation that is more "explainable" than the others is generated.

5.  The structure of an equation generated with symbolic regression
    *will* withstand some noise.

6.  Piping the "winning" equation through a symbolic mathematics package
    to simplify and consolidate it is a good idea.  The effect on the deterministic equations is very slight.

7.  This consolidation often introduces relationships between variables
    and constants that were not specified before the algorithm was
    started. This can nudge the human to reconsider relationships
    previously rejected.

8.  All Symbolic Regression libraries using evolutionary algorithms are
    *not* created equal. Library comparison is beyond the scope of this
    report, but we can comment that `PySR`was one of the better ones.

9.  Running both evolutionary and deterministic models on the same problem is *also* a good idea.

[^2]: A quick note for humans on this second equation: ${log(T_b)}^3 = {(ln(T_b))}^3 \neq ln(T^3_b)$  This was made clear by inspecting the Polish Notation version of the equation.  Just sayin'.

## References
